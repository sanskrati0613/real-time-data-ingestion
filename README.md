# Real-Time Data Ingestion Pipeline with PySpark and Delta Lake

## Project Overview
This project implements a real-time data ingestion pipeline using PySpark and Delta Lake in Google Colab, designed to simulate a scalable, automated data processing system. The pipeline generates synthetic customer data (Name, Address, Email) using the Faker library, stores it in a Delta Lake table, and supports scheduled data ingestion with email notifications. It processes 5 records per iteration, with a configurable 2-minute interval. The project showcases proficiency in real-time data pipelines, Delta Lake versioning, and automated reporting, preparing me for enterprise-grade data engineering tasks.

## Prerequisites
- **Python 3.x**
- **Libraries**:
  - `pyspark==3.4.0`
  - `delta-spark==2.4.0`
  - `faker==19.6.2`
  - `schedule==1.2.0`
  - `pandas`, `smtplib`, `email` (for notifications)
- **Java**: OpenJDK 8 (required for Spark)
- **Google Colab**: Execution environment
- **Dataset**: Synthetic customer data generated programmatically

## Features
Real-time Data Generation: Uses Faker to generate realistic-looking customer data (Name, Address, Email) on a scheduled basis.

Delta Lake Integration: Stores ingested data in a Delta Lake table, providing ACID properties (Atomicity, Consistency, Isolation, Durability), schema enforcement, and versioning capabilities.

PySpark Processing: Utilizes Apache Spark (via PySpark) for efficient data manipulation and writing to the Delta table.

Scheduled Ingestion: Employs the schedule library to run data ingestion iterations at defined intervals.

Email Notifications: Configurable email alerts (using SMTP) to send execution summaries and newly ingested data snapshots.

Google Colab Optimized: Designed to run seamlessly within a Google Colab environment, with necessary dependency installations and environment variable configurations.

Interactive Outputs: Displays newly generated data, latest table contents, and Delta table version information directly in the Colab notebook.

## Dataset Details
The dataset consists of synthetic customer records generated by the Faker library, with the following schema:
- **Columns**:
  - `Name`: Full name (e.g., "Victoria Ryan")
  - `Address`: Full address (e.g., "9761 Sparks Cove, Jenniferberg, CO 73294")
  - `Email`: Email address (e.g., "beckmichael@example.com")
  - `created_timestamp`: Record creation time (ISO format, UTC)
  - `ingestion_timestamp`: Data ingestion time (added during pipeline execution)

- **Generation**: 5 records per iteration, configurable via `records_per_iteration`

## Implementation Steps
1. **Environment Setup**:
   - Installed dependencies (`pyspark`, `delta-spark`, `faker`, `schedule`) and OpenJDK 8.
   - Configured Spark with Delta Lake extensions for efficient data storage.
2. **Pipeline Configuration**:
   - Defined settings for Delta table path (`/content/delta-table/customer_data`), timezone (`Asia/Kolkata`), and email notifications (Gmail SMTP).
3. **Pipeline Initialization**:
   - Created a `ColabDeltaDataPipeline` class to manage Spark session, data generation, and Delta Lake operations.
   - Initialized Spark with configurations for memory, serialization, and timezone.
4. **Data Generation**:
   - Generated 5 synthetic records per iteration using Faker, including Name, Address, Email, and `created_timestamp`.
5. **Delta Lake Operations**:
   - Created a Delta table if it doesn't exist, with an initial record to define the schema.
   - Appended new data to the Delta table with `ingestion_timestamp`.
   - Maintained version history for auditing and rollback.
6. **Scheduled Execution**:
   - Ran the pipeline every 2 minutes for a 10-minute duration using the `schedule` library and threading.
   - Supported manual iteration and stop controls.
7. **Notifications and Visualization**:
   - Sent HTML email notifications with execution summaries and data previews using Gmail SMTP.
   - Displayed data and version information in HTML tables within Colab.
8. **Data Analysis**:
   - Provided basic analysis (total records, unique names/emails, date range) using Pandas.
   - Listed Delta table files for transparency.

## Key Features
- **Real-Time Ingestion**: Processes 5 records every 2 minutes, simulating real-time data streams.
- **Delta Lake Integration**: Ensures ACID transactions, versioning, and scalability.
- **Automation**: Scheduled execution with threading reduces manual intervention.
- **Email Notifications**: Sends styled HTML reports with data summaries and previews.
- **Visualization**: Displays data and version info in HTML tables for easy monitoring in Colab.
- **Error Handling**: Robust exception handling for Spark, Delta Lake, and email operations.

## Challenges and Solutions
- **Challenge**: Running scheduled tasks in Google Colab's ephemeral environment.
  - **Solution**: Used threading with a fixed duration (10 minutes) and the `schedule` library to simulate continuous execution.
- **Challenge**: Configuring email notifications with Gmail's security settings.
  - **Solution**: Utilized an app-specific password (`onoh sijk vxpw wijq`) for secure SMTP access.
- **Challenge**: Managing Delta Lake in a non-cloud environment.
  - **Solution**: Configured Delta Lake with local storage (`/content/delta-table/customer_data`) and optimized Spark settings for Colab.
- **Challenge**: Displaying large datasets in a user-friendly format.
  - **Solution**: Created HTML tables with truncation (e.g., addresses limited to 50 characters) and limited previews to 10 records.

## Project Artifacts
- **Code**: [GitHub Repository][(https://github.com/sanskrati0613/real-time-data-ingestion.git)]
- **Notebook**: `Real_Time_Data_ingestion.ipynb` (see repository)
- **Email Screenshot**: (https://drive.google.com/drive/folders/1DrEQBwHb1D-eCDrWfAMsbAKiXa-7o1rY?usp=sharing)

## How to Run
1. Clone the repository: `git clone (https://github.com/sanskrati0613/real-time-data-ingestion.git)`
2. Open `Real_Time_Data_ingestion.ipynb` in Google Colab.
3. Install dependencies:
   ```bash
   !pip install pyspark==3.4.0 delta-spark==2.4.0 faker==19.6.2 schedule==1.2.0
   !apt-get update -qq
   !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null





Configure environment variables:

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"



Update email settings in the config dictionary (if notifications are desired):





Set sender_email, sender_password (app-specific password), and recipient_emails.



Execute notebook cells to initialize the pipeline, run iterations, and view results.



Run pipeline.cleanup() to stop the Spark session when done.

Future Improvements





Deploy the pipeline in a cloud environment (e.g., Databricks) for production-grade execution.



Add data quality checks (e.g., email format validation, duplicate detection).



Integrate with Apache Kafka for true real-time streaming.



Enhance visualizations with charts (e.g., record growth over time).



Implement monitoring with logging frameworks (e.g., ELK stack).

Contact





Author: Sanskrati Jain



Email: sanskratijain88@gmail.com



LinkedIn: www.linkedin.com/in/sanskrati-jain-295b65271



GitHub: [github.com/sanskrati0613](https://github.com/sanskrati0613)

 ## Google colab link: 
 [https://colab.research.google.com/drive/1zft3WrkdcalzSir3W_ltSmAOagBqInDr#scrollTo=T1UwyhP5p0QZ]
